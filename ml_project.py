# -*- coding: utf-8 -*-
"""ML_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuGW3nmK9G2cSEBlT8N00-86H_6JFatm

1. Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

import os
print(os.listdir())

import warnings
warnings.filterwarnings('ignore')

"""2. Importing and understanding the dataset"""

# read the csv file
df=pd.read_csv('/content/drive/MyDrive/heart.csv')

#Verifying it as a dataframe object in pandas
type(df)

# shape of dataset
df.shape

# top 5 columns
df.head()

# describing the dataset
df.describe()

# to check null values are there or not
df.info()

# understanding our columns clearly
info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]



for i in range(len(info)):
    print(df.columns[i]+":\t\t\t"+info[i])

# analysing the target variable
df['target'].describe()

df['target'].unique()

"""Clearly, this is a classification problem, with the target variable having values '0' and '1'.

Checking correlation between columns
"""

print(df.corr()["target"].abs().sort_values(ascending=False))

"""Exploratory Data Analysis (EDA)

First analysing the 'target' variable
"""

y = df["target"]
sns.histplot(y)
target_temp = df.target.value_counts()
print(target_temp)

# print("Percentage of patience with heart problems: "+str(y.where(y==1).count()*100/303))
# print("Percentage of patience with heart problems: "+str(y.where(y==0).count()*100/303))

# #Or,
# countNoDisease = len(df[df.target == 0])
# countHaveDisease = len(df[df.target == 1])
print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

"""We'll analyse 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca' and 'thal' features.

Analysing the 'Sex' feature
"""

df["sex"].unique()

"""We notice, that as expected, the 'sex' feature has 2 unique features"""

sns.barplot(x=df["sex"], y=y)

"""We notice, that females are more likely to have heart problems than males

Analysing the 'Chest Pain Type' feature
"""

df["cp"].unique()

"""As expected, the CP feature has values from 0 to 3"""

sns.barplot(x=df["cp"],y=y)

"""We notice, that chest pain of '0', i.e. the ones with typical angina are much less likely to have heart problems

Analysing the FBS feature
"""

df["fbs"].describe()

df["fbs"].unique()

sns.barplot(x=df["fbs"],y=y)

"""Nothing extraordinary here

Analysing the restecg feature
"""

df["restecg"].unique()

sns.barplot(x=df["restecg"],y=y)

"""We realize that people with restecg '1' and '0' are much more likely to have a heart disease than with restecg '2'.

Analysing the 'exang' feature
"""

df['exang'].unique()

sns.barplot(x=df['exang'],y=y)

"""People with exang=1 i.e. Exercise induced angina are much less likely to have heart problems.

Analysing the Slope feature
"""

df['slope'].unique()

sns.barplot(x=df['slope'],y=y)

"""We observe, that Slope '2' causes heart pain much more than Slope '0' and '1'.

Analysing the 'ca' feature
"""

df['ca'].unique()

sns.countplot(x=df['ca'],hue=y)

sns.barplot(x=df['ca'],y=y)

"""ca=4 has astonishingly large number of heart patients"""

df['thal'].unique()

sns.barplot(x=df['thal'],y=y)

sns.distplot(df['thal'])

"""4. Train Test Split"""

from sklearn.model_selection import train_test_split

predictors = df.drop("target",axis=1)
target = df["target"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

"""5. Model Fitting"""

from sklearn.metrics import accuracy_score

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train,Y_train)

Y_pred_lr = lr.predict(X_test)

Y_pred_lr.shape

score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using Logistic Regression is: "+str(score_lr)+" %")

"""NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(X_train,Y_train)

Y_pred_nb = nb.predict(X_test)

Y_pred_nb.shape

score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)

print("The accuracy score achieved using Naive Bayes is: "+str(score_nb)+" %")

"""SUPPORT VECTOR MACHINE(SVM)"""

from sklearn import svm

sv = svm.SVC(kernel='linear')

sv.fit(X_train, Y_train)

Y_pred_svm = sv.predict(X_test)

Y_pred_svm.shape

score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)

print("The accuracy score achieved using Linear SVM is: "+str(score_svm)+" %")

"""K NEAREST NEIGHBOURS"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train,Y_train)

Y_pred_knn=knn.predict(X_test)

Y_pred_knn.shape

score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)

print("The accuracy score achieved using KNN is: "+str(score_knn)+" %")

"""DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier

max_accuracy = 0


for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)
    dt.fit(X_train,Y_train)
    Y_pred_dt = dt.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x

#print(max_accuracy)
#print(best_x)


dt = DecisionTreeClassifier(random_state=best_x)
dt.fit(X_train,Y_train)
Y_pred_dt = dt.predict(X_test)

Y_pred_dt.shape

score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_dt)+" %")

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

max_accuracy = 0


for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x

#print(max_accuracy)
#print(best_x)

rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)

Y_pred_rf.shape

score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)

print("The accuracy score achieved using Random Forest is: "+str(score_rf)+" %")

"""XGBoost"""

import xgboost as xgb

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, Y_train)

Y_pred_xgb = xgb_model.predict(X_test)

Y_pred_xgb.shape

score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)

print("The accuracy score achieved using XGBoost is: "+str(score_xgb)+" %")

"""6. OUTPUT FINAL SCORE"""

scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb]
algorithms = ["Logistic Regression","Naive Bayes","Support Vector Machine","K-Nearest Neighbors","Decision Tree","Random Forest","XGBoost"]

for i in range(len(algorithms)):
    print("The accuracy score achieved using "+algorithms[i]+" is: "+str(scores[i])+" %")

sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")



























sns.barplot(x=algorithms,y=scores)

"""Random Forest has good result as compare to other algorithms"""